{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtDJmBNAc-sm"
      },
      "source": [
        "# Aula 6 - Regressão Logística"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mZhb5hJwMjir"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf8\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Qz8JaO7OY4Ke"
      },
      "outputs": [],
      "source": [
        "from numpy.testing import assert_almost_equal\n",
        "from numpy.testing import assert_equal\n",
        "from numpy.testing import assert_array_almost_equal\n",
        "from numpy.testing import assert_array_equal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUcs3q2IMji5"
      },
      "source": [
        "## Breast Cancer Dataset\n",
        "\n",
        "Esse dataset contém dados de pacientes com tumores de mama, você tentará prever se um tumor é maligno ou benigno de acordo com a espessura do tumor. No gráfico abaixo você pode visualizar os tipos de tumores (1 = maligno, 0 = benigno) por categoria de espessura."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-ilYEm2Maur"
      },
      "source": [
        "|Index | Attribute | Domain |\n",
        "|----|----|----|\n",
        "0|Sample code number | id number\n",
        "1|Clump Thickness | 1 - 10\n",
        "2|Uniformity of Cell Size | 1 - 10\n",
        "3|Uniformity of Cell Shape | 1 - 10\n",
        "4|Marginal Adhesion | 1 - 10\n",
        "5|Single Epithelial Cell Size | 1 - 10\n",
        "6|Bare Nuclei | 1 - 10\n",
        "7|Bland Chromatin | 1 - 10\n",
        "8|Normal Nucleoli | 1 - 10\n",
        "9|Mitoses | 1 - 10\n",
        "10|Class | (2 for benign, 4 for malignant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R_d4xgMQJeJ"
      },
      "source": [
        "### Pré-processamento dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-8KKQesHP18i"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/'\\\n",
        "                'breast-cancer-wisconsin/breast-cancer-wisconsin.data',\n",
        "                header=None)\n",
        "# o nosso dataset contém strings `?` no lugar de dados faltantes\n",
        "# vamos substituir esses valores para não termos problemas na execução\n",
        "# das abordagens numéricas\n",
        "#df.replace('?', np.NaN, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "iE-VkdfgmZ9V"
      },
      "outputs": [],
      "source": [
        "df.replace('?', np.NaN, inplace=True)\n",
        "scaled_df = df.iloc[:, 1:-1].copy()\n",
        "scaled_df = scaled_df.drop(columns=6)\n",
        "scaled_df = (scaled_df - scaled_df.mean())/scaled_df.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Rouz43kM5O1N"
      },
      "outputs": [],
      "source": [
        "X = scaled_df\n",
        "X = np.nan_to_num(X)\n",
        "y = df.iloc[:, -1]\n",
        "y = y.values\n",
        "y[y==2] = 0\n",
        "y[y==4] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i09BX2XUQYtz"
      },
      "source": [
        "Enfim, vamos obter os conjuntos de treino e teste para os algoritmos que vamos implementar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "y7slfTW2Capn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \\\n",
        "                                                    random_state=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJloWAEWNcoa"
      },
      "source": [
        "## Regressão Logística\n",
        "\n",
        "Como discutido na aula, a regressão logística baseia-se na função sigmoide:\n",
        "\n",
        "$$f_\\theta(x) = \\frac{1}{1+e^{-\\theta x_i}}$$\n",
        "\n",
        "À medida que sua entrada se torna grande e positiva, $f(x)$ se aproxima e se aproxima de 1. À medida que sua entrada se torna grande e negativa, $f(x)$ se aproxima e se aproxima de 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax5uHzgBSTRx"
      },
      "source": [
        "### Aquecimento 1\n",
        "\n",
        "Você consegue implementar a função `sigmoid` abaixo?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "rR1uUagKBP3N"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z, theta):\n",
        "    return 1 / (1 + np.exp(-np.dot(z, theta)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HRJSZwWQWFc"
      },
      "source": [
        "Além disso, $f(x)$ tem a propriedade conveniente que sua derivada é dada por:\n",
        "\n",
        "$$f_\\theta'(x) = \\frac{e^{-x}}{(1+e^{-\\theta x_i})^2} = f(x)(1-f(x))$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF901P9TOakE"
      },
      "source": [
        "Vamos testar a função implementada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-HOF8nvOa00"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-5,5,100)\n",
        "theta = 1\n",
        "y = sigmoid(x, theta)\n",
        "plt.plot(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpQW3-G0T3Ne"
      },
      "source": [
        "### Aquecimento 2\n",
        "\n",
        "Você consegue implementar a função `sigmoid_prime` abaixo?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-UyTU-iVSqon"
      },
      "outputs": [],
      "source": [
        "def sigmoid_prime(z, theta):\n",
        "  sig_z = sigmoid(z, theta)\n",
        "  return sig_z * (1 - sig_z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUTXWtETIcng"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, sigmoid_prime(x, theta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z0dYKBkJQuK"
      },
      "source": [
        "Vamos usar a função sigmoide para ajustar um modelo:\n",
        "\n",
        "$$y_i = f(x_i\\theta) + \\epsilon_i$$\n",
        "\n",
        "onde $f$ é a função logística (`sigmoid`).\n",
        "\n",
        "Note também que $x_i\\theta$, para $j$ variáveis independentes, nada mais é que o modelo linear visto nas aulas anteriores, que é calculado e dado como entrada para a função logística:\n",
        "\n",
        "$$x_i\\theta = \\theta_0 + \\theta_1 x_1 + \\cdots + \\theta_j x_j$$\n",
        "\n",
        "Ao contrário do que acontece no caso da Regressão Linear, precisamos calcular diretamente a função de verossimilhança e seu gradiente. Porém, vimos que é mais simples maximizar o logaritmo da verossimilhança (*log likelihood*):\n",
        "\n",
        "$$\\log ll_{\\theta}(y_i~|~x_i) = y_i \\log f(x_i\\theta) + (1-y_i) \\log (1-f(x_i\\theta))$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAGT8FysJQuK"
      },
      "source": [
        "### Cross Entropy\n",
        "\n",
        "Ao invés de trabalhar na verossimilhança, vamos invertê-la e utilizar o *cross entropy* para a regressão logística.\n",
        "\n",
        "$$L(\\theta) = -n^{-1}\\sum_i \\big((1-y_i)\\log_2(1-f_{\\theta}(x_i)) + y_i\\log_2(f_{\\theta}(x_i))\\big)$$\n",
        "\n",
        "A equação acima é a cross-entropy média por observação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J-4Nps9b4Gn"
      },
      "source": [
        "#### Aquecimento 3\n",
        "\n",
        "Você consegue implementar a cross-entropy média na função `cross_entropy_mean` abaixo?\n",
        "\n",
        "Dica: você pode utilizar a função `np.clip(vetor, limite_inferior, limite_superior)` para evitar imprecisões numéricas quando calcular logarítimos ou realizar divisões.\n",
        "\n",
        "Por exemplo, o exemplo abaixo limita os valores do `vetor_exemplo` entre 0.0001 e 0.9999. Ou seja, se o vetor tiver um valor 1.01 por erro numérico, corrigimos para 0.9999.\n",
        "\n",
        "```\n",
        "vetor = [0.00, 0.01, 1.02]\n",
        "novo_vetor = np.clip(vetor, 0.00001, 0.99999)\n",
        "# novo_vetor: [0.0001, 0.01, 0.9999]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "N4VoovXVb3lO"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_mean(X, y, theta):\n",
        "    yp = y > 0.5\n",
        "    logit = sigmoid(X, theta)\n",
        "    logit = np.clip(logit, 0.00001, 0.99999)\n",
        "    return -(yp * np.log(logit) + (1 - yp) * np.log(1 - logit)).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2R2B8g_cUUZ"
      },
      "source": [
        "### Derivada de $L(\\theta)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R0f0yPnWJ-w"
      },
      "source": [
        "A derivada de $L(\\theta)$ tem uma forma similar ao da regressão linear. Veja a derivação nos [slides](https://docs.google.com/presentation/d/1yGPETPe8o7PPOP6_CF38LHr3vpxgTEnF5LjP-1pkGIc/edit?usp=sharing). Partindo da derivada $f_\\theta'(x)$, definida acima, chegamos em:\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "L'(\\theta) = -n^{-1}\\sum_i \\big(-\\frac{(1-y_i)f'_{\\theta}(x_i)}{1- f_{\\theta}(x_i)} + \\frac{y_if'_{\\theta}(x_i) }{f_{\\theta}(x_i)}\\big)\n",
        "$$\n",
        "\n",
        "Simplificando:\n",
        "\n",
        "$$\n",
        "L'(\\theta) = -n^{-1}\\sum_i \\big(-\\frac{(1-y_i)f'_{\\theta}(x_i)}{1- f_{\\theta}(x_i)} + \\frac{y_if'_{\\theta}(x_i) }{f_{\\theta}(x_i)}\\big) \\\\\n",
        "L'(\\theta) = -n^{-1}\\sum_i \\big(-\\frac{(1-y_i)}{1- f_{\\theta}(x_i)} + \\frac{y_i}{f_{\\theta}(x_i)}\\big)f_{\\theta}(x_i)(1-f_{\\theta}(x_i)) \\\\\n",
        "L'(\\theta) = -n^{-1}\\sum_i \\big(-\\frac{(1-y_i)}{1- f_{\\theta}(x_i)} + \\frac{y_i}{f_{\\theta}(x_i)}\\big)f_{\\theta}(x_i)(1-f_{\\theta}(x_i)) \\\\\n",
        "L'(\\theta) = -n^{-1}\\sum_i (y_i - f_{\\theta}(x_i)) x_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZoU6hjgVsU7"
      },
      "source": [
        "#### Aquecimento 4\n",
        "\n",
        "Você consegue escrever a forma vetorizada de $L'(\\theta)$ na função `derivadas` abaixo?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "wlFKhJU0VZLI"
      },
      "outputs": [],
      "source": [
        "def derivadas(theta, X, y):\n",
        "  n = len(y)\n",
        "  predictions = sigmoid(X, theta)\n",
        "  error = y - predictions\n",
        "  gradient = -1/n * np.dot(X.T, error)\n",
        "  return gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHHZyQEuMjjI"
      },
      "source": [
        "## Gradiente Descendente\n",
        "\n",
        "Na Aula 5, utilizamos o gradiente descendente no contexto da Regressão Linear. Agora, vamos aplicar o gradiente descendente para o caso da Regressão Logística. A ideia aqui é a mesma:\n",
        "\n",
        "1. Escolha um valor inicial do vetor (e.g., θ = [0.2, 1, -2, 5])\n",
        "2. Escolha uma taxa de aprendizado (e.g., λ = 0.01)\n",
        "3. Repita:\n",
        "  1. Compute a derivada da log-verossimilhança (log likelihood) ll'(θ)\n",
        "  2. Atualize $\\theta$\n",
        "4. Pare quando convergir\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ItwyTasYWoo"
      },
      "source": [
        "#### Exercício 1\n",
        "\n",
        "Modifique a implementação de `gd` para o caso da regressão logística."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "o63EA8W4MjjJ"
      },
      "outputs": [],
      "source": [
        "def gd(X, y, lambda_=0.01, tol=0.0000001, max_iter=10000):\n",
        "    theta = np.ones(X.shape[1])\n",
        "    # print('Iter {}; theta = '.format(0), theta)\n",
        "    old_err = np.inf\n",
        "    i = 0\n",
        "    while True:\n",
        "        # Computar as derivadas. Modifique a linha 8 abaixo\n",
        "        grad = derivadas(theta, X, y)\n",
        "        # Atualizar\n",
        "        theta_novo = theta - lambda_ * grad\n",
        "        # Parar quando o erro convergir.\n",
        "        # Modifique a linha 13 abaixo.\n",
        "        err = cross_entropy_mean(X, y, theta)\n",
        "        if np.abs(old_err - err) <= tol:\n",
        "            break\n",
        "        theta = theta_novo\n",
        "        old_err = err\n",
        "        # print('Iter {}; theta = {}; cross_e = {}'.format(i+1, theta, err))\n",
        "        i += 1\n",
        "        if i == max_iter:\n",
        "            break\n",
        "    return theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msQV2vIhUbrw"
      },
      "source": [
        "Testando:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "K18t5pIFB4tX"
      },
      "outputs": [],
      "source": [
        "theta = gd(X_train, y_train)\n",
        "r2_train = cross_entropy_mean(X_train, y_train, theta)\n",
        "r2_test = cross_entropy_mean(X_test, y_test, theta)\n",
        "assert_equal(np.round(r2_train,2), np.round(0.08605687702630486,2))\n",
        "assert_equal(np.round(r2_test,2), np.round(0.18899648326443957,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCSsodiupa4P"
      },
      "source": [
        "## Gradiente Descendente Estocástico\n",
        "\n",
        "Relembrando a Aula 3:\n",
        "> Muitas vezes usaremos o gradiente descendente para escolher os parâmetros de um modelo de maneira a minimizar alguma noção de erro. Usando a abordagem em lote anterior, cada etapa do método exige que façamos uma previsão e calculemos o gradiente para todo o conjunto de dados, o que faz com que cada etapa demore muito. <br>\n",
        "> <br> Geralmente, essas funções de erro são *aditivas*, o que significa que o erro preditivo em todo o conjunto de dados é simplesmente a soma dos erros preditivos para cada ponto de dados. <br>\n",
        "> <br> Quando este é o caso, podemos, em vez disso, aplicar uma técnica chamada gradiente descendente estocástico (ou *stochastic gradient descent*), que calcula o gradiente (e dá um passo) para apenas um ponto por vez. Ele passa sobre os dados repetidamente até atingir um ponto de parada.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCMACFYmaTs6"
      },
      "source": [
        "### Exercício 2\n",
        "\n",
        "Modifique a implementação de `sgd` para o caso da regressão logística."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "K0AM_EA8Z5BP"
      },
      "outputs": [],
      "source": [
        "def sgd(X, y, lambda_=0.001, tol=0.01, max_iter=10000):\n",
        "    theta = np.ones(X.shape[1])\n",
        "    # theta = theta.copy()\n",
        "    # print('Iter {}; alpha, beta = '.format(0), theta)\n",
        "    old_err_sq = np.inf\n",
        "    for i in range(max_iter):\n",
        "        # Escolhe ponto aleatório\n",
        "        r = np.random.randint(len(y))\n",
        "        X_r, y_r = X[r], y[r]\n",
        "        X_r = X_r.reshape(1, len(X_r)) # transforma o vetor linha em matriz\n",
        "\n",
        "        # Deriva e atualiza\n",
        "        # Modifique a linha 14 abaixo.\n",
        "        grad = derivadas(theta, X, y)\n",
        "        theta_novo = theta - lambda_ * grad\n",
        "\n",
        "        # Calcula o erro\n",
        "        # Modifique a linha 19 abaixo.\n",
        "        err_sq = cross_entropy_mean(X_r, y_r, theta)\n",
        "\n",
        "        theta = theta_novo\n",
        "        if err_sq < tol:\n",
        "            break\n",
        "\n",
        "        #print('Iter {}; alpha, beta = '.format(i+1), theta)\n",
        "\n",
        "    return theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKtSBLVwUtzV"
      },
      "source": [
        "Testando:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "v8fceHuJFd3Q"
      },
      "outputs": [],
      "source": [
        "theta = sgd(X_train, y_train)\n",
        "\n",
        "r2_train = cross_entropy_mean(X_train, y_train, theta)\n",
        "r2_test = cross_entropy_mean(X_test, y_test, theta)\n",
        "assert_equal(np.round(r2_train,2), np.round(0.09296116832663982,2))\n",
        "assert_equal(np.round(r2_test,2), np.round(0.19610717861042887,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GswPqf8xKVmU"
      },
      "source": [
        "## sklearn: LogisticRegressionCV (somente didatico)\n",
        "\n",
        "Por último, temos um exemplo de regressão logística com regularização L2 utilizando a biblioteca sklearn.\n",
        "\n",
        "O exemplo abaixo usa a versão com validação cruzada, a função *LogisticRegressionCV*, cuja documentação você encontra [aqui](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV). A maior diferença desta implementação para as abordagens que estudamos na Aula 6 é que a LogisticRegressionCV usa, por padrão, um método da família quasi-Newton invés do gradiente descendente, o [Limited-memory BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS).\n",
        "\n",
        "Como grande parte das funções implementadas no sklearn, a LogisticRegressionCV possui vários parâmetros configuráveis, cuja melhor configuração vai depender do problema com o qual se está trabalhando. Dê uma olhada na documentação e tente brincar com as várias combinações de parâmetros!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz6-U1jSKaXX"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "clf = LogisticRegressionCV(penalty='l2', cv=3).fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "print(clf.predict_proba(X_test).shape)\n",
        "# No caso deste modelo, o score padrão computado é a acurácia (0, 1).\n",
        "# Podemos escolher outras funções já implementadas ou\n",
        "# passar a nossa própria métrica como parâmetro no momento da construção\n",
        "# do regressor -> clf = LogisticRegressionCV(..., scoring=metrica_de_interesse)\n",
        "# métricas implementadas:\n",
        "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
        "print(clf.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh5vNc8KKbxt"
      },
      "outputs": [],
      "source": [
        "clf.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hss-IUwKe_z"
      },
      "outputs": [],
      "source": [
        "print(\"Cross-entropy em teste = \", cross_entropy_mean(X_test, y_test, clf.coef_[0]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}